"""
Otimizador de modelos YOLO para acelera√ß√£o de hardware
Exporta modelos para TensorRT (NVIDIA) e OpenVINO (Intel)
"""
import torch
from pathlib import Path
from typing import Optional
from ultralytics import YOLO

from ..config.settings import config_manager
from ..utils.logger import log_system_event, log_error


def check_and_export_models() -> dict:
    """
    Verifica hardware dispon√≠vel e exporta modelos otimizados se necess√°rio.

    Returns:
        dict: Informa√ß√µes sobre os modelos exportados
    """
    cfg = config_manager.config.detection
    results = {
        'tensorrt': False,
        'openvino': False,
        'directml': False,
        'base_model': False
    }

    # Verifica se o modelo base existe
    model_path = Path(cfg.model_path)
    if not model_path.exists():
        print(f"‚ùå [ModelOptimizer] Modelo base n√£o encontrado: {cfg.model_path}")
        print(f"   Por favor, coloque o arquivo 'best.pt' na pasta 'modelos/'")
        return results

    results['base_model'] = True
    print(f"‚úÖ [ModelOptimizer] Modelo base encontrado: {cfg.model_path}")

    # Se auto_optimize estiver desabilitado, pula exporta√ß√£o
    if not cfg.auto_optimize:
        print("‚ÑπÔ∏è [ModelOptimizer] Auto-otimiza√ß√£o desabilitada no config.json")
        return results

    # 1. Verifica e exporta TensorRT (NVIDIA CUDA)
    if _check_and_export_tensorrt(cfg, model_path):
        results['tensorrt'] = True

    # 2. Verifica e exporta OpenVINO (Intel)
    if _check_and_export_openvino(cfg, model_path):
        results['openvino'] = True

    # 3. Verifica DirectML (AMD/Outras GPUs no Windows)
    if _check_directml():
        results['directml'] = True

    return results


def _check_and_export_tensorrt(cfg, model_path: Path) -> bool:
    """Verifica GPU NVIDIA e exporta modelo para TensorRT"""
    tensorrt_path = Path(cfg.model_path_tensorrt)

    # Verifica se CUDA est√° dispon√≠vel
    if not torch.cuda.is_available():
        print("‚ÑπÔ∏è [TensorRT] GPU NVIDIA n√£o detectada (CUDA n√£o dispon√≠vel)")
        return False

    # Verifica se o modelo j√° existe
    if tensorrt_path.exists():
        print(f"‚úÖ [TensorRT] Modelo otimizado j√° existe: {tensorrt_path}")
        return True

    # Exporta modelo para TensorRT
    try:
        print("üöÄ [TensorRT] GPU NVIDIA detectada! Exportando modelo...")
        print("   ‚ö†Ô∏è Isso pode levar alguns minutos na primeira execu√ß√£o...")

        model = YOLO(str(model_path))
        model.export(format='engine', device=0)  # TensorRT export

        # Verifica se a exporta√ß√£o foi bem-sucedida
        if tensorrt_path.exists():
            print(f"‚úÖ [TensorRT] Modelo exportado com sucesso: {tensorrt_path}")
            log_system_event("TENSORRT_MODEL_EXPORTED")
            return True
        else:
            print("‚ö†Ô∏è [TensorRT] Exporta√ß√£o conclu√≠da mas arquivo n√£o encontrado")
            return False

    except Exception as e:
        log_error("ModelOptimizer", e, "Erro ao exportar para TensorRT")
        print(f"‚ùå [TensorRT] Falha na exporta√ß√£o: {str(e)}")
        print("   Continuando com CPU/DirectML...")
        return False


def _check_and_export_openvino(cfg, model_path: Path) -> bool:
    """Verifica CPU Intel e exporta modelo para OpenVINO"""
    openvino_path = Path(cfg.model_path_openvino)

    # Verifica se o modelo j√° existe
    if openvino_path.exists() and openvino_path.is_dir():
        print(f"‚úÖ [OpenVINO] Modelo otimizado j√° existe: {openvino_path}")
        return True

    # Exporta modelo para OpenVINO
    # Nota: OpenVINO √© √∫til principalmente para CPUs Intel e iGPUs
    try:
        print("üöÄ [OpenVINO] Exportando modelo para otimiza√ß√£o de CPU/Intel...")
        print("   ‚ö†Ô∏è Isso pode levar alguns minutos na primeira execu√ß√£o...")

        model = YOLO(str(model_path))
        model.export(format='openvino')

        # Verifica se a exporta√ß√£o foi bem-sucedida
        if openvino_path.exists():
            print(f"‚úÖ [OpenVINO] Modelo exportado com sucesso: {openvino_path}")
            log_system_event("OPENVINO_MODEL_EXPORTED")
            return True
        else:
            print("‚ö†Ô∏è [OpenVINO] Exporta√ß√£o conclu√≠da mas pasta n√£o encontrada")
            return False

    except Exception as e:
        log_error("ModelOptimizer", e, "Erro ao exportar para OpenVINO")
        print(f"‚ùå [OpenVINO] Falha na exporta√ß√£o: {str(e)}")
        print("   Continuando com CPU padr√£o...")
        return False


def _check_directml() -> bool:
    """Verifica se DirectML est√° dispon√≠vel (AMD/Outras GPUs no Windows)"""
    try:
        import torch_directml
        if torch_directml.is_available():
            device = torch_directml.device()
            print(f"‚úÖ [DirectML] GPU AMD/Outra detectada: {device}")
            log_system_event("DIRECTML_AVAILABLE")
            return True
    except ImportError:
        print("‚ÑπÔ∏è [DirectML] torch-directml n√£o instalado")
        print("   Para usar GPUs AMD no Windows, instale: pip install torch-directml")
    except Exception as e:
        print(f"‚ÑπÔ∏è [DirectML] N√£o dispon√≠vel: {e}")

    return False


def get_hardware_info() -> dict:
    """
    Retorna informa√ß√µes sobre o hardware dispon√≠vel.

    Returns:
        dict: Informa√ß√µes detalhadas do hardware
    """
    info = {
        'cuda_available': False,
        'cuda_devices': 0,
        'cuda_version': None,
        'directml_available': False,
        'cpu_count': 0,
        'recommended_backend': 'cpu'
    }

    # CUDA (NVIDIA)
    if torch.cuda.is_available():
        info['cuda_available'] = True
        info['cuda_devices'] = torch.cuda.device_count()
        info['cuda_version'] = torch.version.cuda
        info['recommended_backend'] = 'tensorrt'

        for i in range(info['cuda_devices']):
            device_name = torch.cuda.get_device_name(i)
            print(f"   GPU {i}: {device_name}")

    # DirectML (AMD/Outras)
    try:
        import torch_directml
        if torch_directml.is_available():
            info['directml_available'] = True
            if not info['cuda_available']:
                info['recommended_backend'] = 'directml'
    except:
        pass

    # CPU
    info['cpu_count'] = torch.get_num_threads()

    return info


def print_optimization_summary(results: dict) -> None:
    """Imprime resumo da otimiza√ß√£o"""
    print("\n" + "=" * 60)
    print("üìä RESUMO DA OTIMIZA√á√ÉO DE MODELOS")
    print("=" * 60)

    if results.get('base_model'):
        print("‚úÖ Modelo Base (PyTorch): Dispon√≠vel")
    else:
        print("‚ùå Modelo Base (PyTorch): N√ÉO ENCONTRADO")
        return

    if results.get('tensorrt'):
        print("‚úÖ TensorRT (NVIDIA): Exportado e pronto")
    else:
        print("‚è≠Ô∏è  TensorRT (NVIDIA): N√£o dispon√≠vel")

    if results.get('openvino'):
        print("‚úÖ OpenVINO (Intel): Exportado e pronto")
    else:
        print("‚è≠Ô∏è  OpenVINO (Intel): N√£o exportado")

    if results.get('directml'):
        print("‚úÖ DirectML (AMD/Outras): Dispon√≠vel")
    else:
        print("‚è≠Ô∏è  DirectML (AMD/Outras): N√£o dispon√≠vel")

    print("=" * 60 + "\n")


if __name__ == "__main__":
    # Teste standalone
    print("üîç Verificando hardware e modelos...\n")

    # Mostra informa√ß√µes do hardware
    hw_info = get_hardware_info()
    print(f"\nüíª Backend recomendado: {hw_info['recommended_backend'].upper()}")

    # Verifica e exporta modelos
    results = check_and_export_models()

    # Mostra resumo
    print_optimization_summary(results)