# ğŸš€ LAS Cams System v2.0 - AceleraÃ§Ã£o de Hardware

## ğŸ“Œ Resumo das MudanÃ§as

Esta atualizaÃ§Ã£o implementa **detecÃ§Ã£o automÃ¡tica e otimizaÃ§Ã£o de hardware**, permitindo que o sistema use:

- âš¡ **TensorRT** (GPUs NVIDIA) - Performance mÃ¡xima
- ğŸ® **DirectML** (GPUs AMD/Intel) - Boa performance no Windows
- ğŸ”§ **OpenVINO** (CPUs/iGPUs Intel) - Otimizado para Intel
- ğŸ’» **CPU PyTorch** (Fallback) - Funciona em qualquer hardware

## ğŸ“ Arquivos Criados/Modificados

### âœ… Novos Arquivos
```
app/
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ model_optimizer.py          # Exporta modelos otimizados
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py (ATUALIZADO)    # Novas configuraÃ§Ãµes
â”œâ”€â”€ services/
â”‚   â””â”€â”€ detection_service.py (ATUALIZADO)  # SeleÃ§Ã£o inteligente de backend
â”œâ”€â”€ models/
â”‚   â””â”€â”€ entities.py (ATUALIZADO)    # Backend info nas entidades
â””â”€â”€ main_refactored.py (ATUALIZADO) # InicializaÃ§Ã£o com otimizaÃ§Ã£o
```

## ğŸ”§ InstalaÃ§Ã£o

### 1. DependÃªncias Base (jÃ¡ instaladas)
```bash
pip install ultralytics opencv-python customtkinter reportlab
```

### 2. DependÃªncias Opcionais (para aceleraÃ§Ã£o)

#### Para GPUs NVIDIA (TensorRT):
```bash
# Instalar CUDA Toolkit 11.8 ou 12.x
# Download: https://developer.nvidia.com/cuda-downloads

# Instalar PyTorch com CUDA
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118
```

#### Para GPUs AMD no Windows (DirectML):
```bash
pip install torch-directml
```

#### Para CPUs Intel (OpenVINO):
```bash
pip install openvino-dev
```

## ğŸš€ Como Usar

### 1. Primeira ExecuÃ§Ã£o (OtimizaÃ§Ã£o AutomÃ¡tica)

Execute a aplicaÃ§Ã£o normalmente:

```bash
python src/main_refactored.py
```

**O que acontece:**
1. O sistema detecta seu hardware
2. Se tiver GPU NVIDIA â†’ Exporta modelo TensorRT (`.engine`)
3. Se tiver GPU AMD â†’ Usa DirectML automaticamente
4. Se tiver CPU Intel â†’ Exporta modelo OpenVINO (`_openvino_model/`)
5. Sempre mantÃ©m o modelo base PyTorch (`.pt`) como fallback

âš ï¸ **IMPORTANTE:** A primeira execuÃ§Ã£o demora ~5-10 minutos (exportaÃ§Ã£o dos modelos). ApÃ³s isso, Ã© instantÃ¢neo!

### 2. ExecuÃ§Ãµes Seguintes

Nas prÃ³ximas execuÃ§Ãµes, o sistema:
- Detecta os modelos jÃ¡ exportados
- Carrega o melhor automaticamente
- Inicia em segundos

## ğŸ“Š ConfiguraÃ§Ã£o Manual (config.json)

Se quiser ajustar as configuraÃ§Ãµes de hardware:

```json
{
  "detection": {
    "model_path": "modelos/best.pt",
    "model_path_tensorrt": "modelos/best.engine",
    "model_path_openvino": "modelos/best_openvino_model",
    "auto_optimize": true,
    "prefer_gpu": true,
    "confidence_threshold": 0.5,
    "max_detection_failures": 150
  }
}
```

### OpÃ§Ãµes:
- `auto_optimize`: Se `true`, exporta modelos automaticamente na primeira execuÃ§Ã£o
- `prefer_gpu`: Se `true`, prioriza GPU sobre CPU
- `max_detection_failures`: NÃºmero de falhas antes de encerrar thread

## ğŸ¯ Prioridade de Backends

O sistema escolhe nesta ordem:

1. **TensorRT** (NVIDIA CUDA)
   - âœ… Mais rÃ¡pido (~100+ FPS)
   - âœ… Melhor para mÃºltiplas cÃ¢meras
   - âš ï¸ Requer GPU NVIDIA

2. **DirectML** (AMD/Intel Windows)
   - âœ… RÃ¡pido (~30-60 FPS)
   - âœ… Funciona com qualquer GPU no Windows
   - âš ï¸ Requer `torch-directml`

3. **OpenVINO** (Intel CPU/iGPU)
   - âœ… Otimizado para CPUs Intel (~15-30 FPS)
   - âœ… Usa iGPU Intel se disponÃ­vel
   - âš ï¸ Melhor que PyTorch puro

4. **CPU PyTorch** (Fallback)
   - âœ… Funciona em qualquer hardware
   - âš ï¸ Mais lento (~5-15 FPS)

## ğŸ› Troubleshooting

### Erro: "Modelo base nÃ£o encontrado"
```
âŒ [ModelOptimizer] Modelo base nÃ£o encontrado: modelos/best.pt
```
**SoluÃ§Ã£o:** Coloque seu arquivo `best.pt` na pasta `modelos/`

### Erro: "CUDA not available"
```
â„¹ï¸ [TensorRT] GPU NVIDIA nÃ£o detectada (CUDA nÃ£o disponÃ­vel)
```
**SoluÃ§Ã£o:** 
1. Verifique se sua GPU Ã© NVIDIA
2. Instale CUDA Toolkit
3. Reinstale PyTorch com suporte CUDA

### Erro: "Cannot set version_counter"
```
RuntimeError: Cannot set version_counter for inference tensor
```
**SoluÃ§Ã£o:** Este erro foi **corrigido** na v2.0! O sistema agora:
- NÃ£o passa `device=` para DirectML
- Carrega o modelo corretamente em cada backend

### Performance baixa
Se o sistema estiver lento:

1. **Verifique qual backend estÃ¡ sendo usado:**
   - Olhe no console: `ğŸš€ [DetectionService] Usando ...`

2. **Force exportaÃ§Ã£o dos modelos:**
   ```bash
   python -m app.utils.model_optimizer
   ```

3. **Reduza a resoluÃ§Ã£o:**
   ```python
   # No detection_service.py, adicione antes do track:
   frame = cv2.resize(frame, (640, 480))
   ```

## ğŸ“ˆ Performance Esperada

| Hardware | Backend | FPS Esperado | MÃºltiplas CÃ¢meras |
|----------|---------|--------------|-------------------|
| RTX 3060 | TensorRT | 100-150 | âœ… Excelente (4-6 cÃ¢meras) |
| RX 6600 | DirectML | 30-60 | âœ… Bom (2-3 cÃ¢meras) |
| i7-12700 | OpenVINO | 15-30 | âš ï¸ Moderado (1-2 cÃ¢meras) |
| i5-8400 | CPU | 5-15 | âŒ Limitado (1 cÃ¢mera) |

## ğŸ” Logs e Debug

Para ver informaÃ§Ãµes detalhadas:

```python
# Ativar verbose no YOLO (detection_service.py)
resultados = model.track(
    frame,
    conf=cfg.confidence_threshold,
    persist=True,
    verbose=True  # Mude para True
)
```

Para ver logs do sistema:
```bash
# Os logs ficam em: logs/app.log
tail -f logs/app.log
```

## ğŸ“ Suporte

Se encontrar problemas:

1. Verifique os logs em `logs/app.log`
2. Execute o teste de hardware:
   ```bash
   python -m app.utils.model_optimizer
   ```
3. Compartilhe a saÃ­da no console

## ğŸ‰ PrÃ³ximos Passos

Agora que o sistema estÃ¡ otimizado:

1. âœ… Teste com suas cÃ¢meras RTSP reais
2. âœ… Ajuste o `confidence_threshold` no config.json
3. âœ… Configure mÃºltiplas cÃ¢meras
4. âœ… Monitore a performance

**Aproveite a velocidade da GPU! ğŸš€**